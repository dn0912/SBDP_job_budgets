service: lambda-gsd-index-calculator

custom:
  REGION: eu-central-1
  S3_BUCKET: test-task-update-data-v2 # put data files here
  S3_SUBRESULT_FOLDER: test_results # subresults of the preprocessing lambdas are stored here
  S3_PIPELINE_RESULT_FOLDER: gsd # final results of the calculation lambdas are stored here
  SQS_NAME: PreprocessedDataQueue
  SQS_NAME_FIFO: PreprocessedDataQueueFiFo
  SQS_NAME_XRAY: PreprocessedDataQueueXRay
  SQS_NAME_REDIS: PreprocessedDataQueueRedis
  S3_FILE: test_pretty.json
  S3_FILE_5TASKS: test_5tasks.json
  S3_PREPROCESS_1000TASKS: test_with_description_title_change_1000_single.json
  REDIS_HOST: '18.156.33.173' # either pass HOST, PORT and PASSWORD or through REDIS_CONNECTION
  REDIS_PORT: 6379
  REDIS_PASSWORD: 'redis-password'
  REDIS_CONNECTION: 'redis://:redis-password@18.156.33.173:6379' # necessary for tracer to store in Redis

provider:
  name: aws
  runtime: nodejs12.x
  region: ${self:custom.REGION}
  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:GetObject
        - s3:PutObject
        - s3:ListBucket
        - s3:DeleteObject
      Resource:
        - arn:aws:s3:::${self:custom.S3_BUCKET}
        - arn:aws:s3:::${self:custom.S3_BUCKET}/*
    - Effect: Allow
      Action:
        - apigateway:POST
      Resource:
        - arn:aws:apigateway:::/*
    - Effect: Allow
      Action:
        - sqs:*
      Resource:
        - arn:aws:sqs:*:*:${self:custom.SQS_NAME}
        - arn:aws:sqs:*:*:${self:custom.SQS_NAME_XRAY}
        - arn:aws:sqs:*:*:${self:custom.SQS_NAME_REDIS}
        - arn:aws:sqs:*:*:${self:custom.SQS_NAME_FIFO}.fifo
        # - arn:aws:sqs:*:*:*
    - Effect: Allow
      Action:
        - lambda:InvokeFunction
      Resource:
        - arn:aws:lambda:${self:custom.REGION}:*:function:*
    - Effect: Allow
      Action:
        # - xray:*
        # below for tracing in sls
        - xray:PutTraceSegments
        - xray:PutTelemetryRecords
        - xray:GetSamplingRules
        - xray:GetSamplingTargets
        - xray:GetSamplingStatisticSummaries
      Resource:
        - "*"
  timeout: 30
  tracing:
    # apiGateway: true
    lambda: true
    # s3: true


functions:
  # *******************
  # usefull helper functions
  cleanup:
    description: cleanup S3 directories
    handler: src/helper.cleanup
    events:
      - http: GET cleanup
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      SUBRESULT_FOLDER: ${self:custom.S3_SUBRESULT_FOLDER}
      PIPELINE_RESULT_FOLDER: ${self:custom.S3_PIPELINE_RESULT_FOLDER}
  # upload-test-data:
  #   description: creates test data and uploads it to S3 for further processing
  #   handler: src/test-data-generator
  #   events:
  #     - http: PUT generate
  #   environment: 
  #     BUCKET: ${self:custom.S3_BUCKET}
  # *******************
  # FIRST ITERATION: SINGLE LAMBDA
  calculate:
    description: Single lambda - reads S3 file, filters and calculates GSD index function in one run
    handler: handler.calculateGSDIndex
    events:
      - http: GET calculate
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      FILE: ${self:custom.S3_FILE}
  calculate5:
    description: >
      Same as calculate function:
      Single lambda - 1. reads S3 file (data consists of only 5 tasks + tasks updates),
      2. filters and 3. calculates GSD index function - test data consists of 5 tasks
    handler: handler.calculateGSDIndex
    events:
      - http: GET calculate5
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      FILE: ${self:custom.S3_FILE_5TASKS}
  # *******************
  # SECOND ITERATION: INTERCONNECTED LAMBDAS WITH SQS AND S3
  preprocess1k:
    description: >
      Preprocess lambda which sends SQS message afterwards:
      preprocess task update data (test data consists of 1000 tasks)
    handler: src/preprocessor.readAndFilterFile
    events:
      - http: 
          path: preprocess1k
          method: post
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      FILE: ${self:custom.S3_PREPROCESS_1000TASKS}
      SUBRESULT_FOLDER: ${self:custom.S3_SUBRESULT_FOLDER}
      REGION: ${self:custom.REGION}
      QUEUE_NAME: ${self:custom.SQS_NAME}
      # QUEUE_NAME: ${self:custom.SQS_NAME_FIFO}.fifo # # TODO: enable for FIFO, must end with fifo suffix
      REDIS_CONNECTION: ${self:custom.REDIS_CONNECTION}
  calculate1k:
    description: >
      Single lambda - calulator lambda after receiving sqs message,
      fetch cleaned up test data from S3 and calculates GSD index
    handler: src/calculator.handler
    events:
      - sqs:
          arn:
            Fn::GetAtt:
              - PreprocessedDataQueue
              # - PreprocessedDataQueueFiFo # TODO: enable for FIFO
              - Arn
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      SUBRESULT_FOLDER: ${self:custom.S3_SUBRESULT_FOLDER}
      PIPELINE_RESULT_FOLDER: ${self:custom.S3_PIPELINE_RESULT_FOLDER}
      REDIS_CONNECTION: ${self:custom.REDIS_CONNECTION}
  # *******************
  # THIRD ITERATION: MANAGER WHICH ORCHESTRATES LAMBDAS IN THE JOB
  start-job:
    description: >
      Single endpoint to trigger a job
    handler: src/job_manager.startJob
    events:
      - http: POST start-job
    environment:
      REDIS_CONNECTION: ${self:custom.REDIS_CONNECTION}
  # *******************
  # XRAY VERSION
  start-job-with-xray:
    description: >
      Single endpoint to trigger a job
    handler: src/xray_version/job_manager-xray.startJob
    events:
      - http: POST start-job-with-xray
  preprocess-with-xray:
    description: >
      Preprocess lambda which sends SQS message afterwards:
      preprocess task update data (test data consists of 1000 tasks)
    handler: src/xray_version/preprocessor-xray.readAndFilterFile
    events:
      - http: 
          path: preprocess-with-xray
          method: post
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      SUBRESULT_FOLDER: ${self:custom.S3_SUBRESULT_FOLDER}
      REGION: ${self:custom.REGION}
      QUEUE_NAME: ${self:custom.SQS_NAME_XRAY}
  calculate-with-xray:
    description: >
      Single lambda - calulator lambda after receiving sqs message,
      fetch cleaned up test data from S3 and calculates GSD index
    handler: src/xray_version/calculator-xray.handler
    events:
      - sqs:
          arn:
            Fn::GetAtt:
              - PreprocessedDataQueueXRay
              - Arn
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      SUBRESULT_FOLDER: ${self:custom.S3_SUBRESULT_FOLDER}
      PIPELINE_RESULT_FOLDER: ${self:custom.S3_PIPELINE_RESULT_FOLDER}
  # *******************
  # REDIS VERSION
  start-job-with-redis:
    description: >
      Single endpoint to trigger a job
    handler: src/redis_version/job_manager-redis.startJob
    events:
      - http: POST start-job-with-redis
    environment:
      REDIS_CONNECTION: ${self:custom.REDIS_CONNECTION}
  preprocess-with-redis:
    description: >
      Preprocess lambda which sends SQS message afterwards:
      preprocess task update data (test data consists of 1000 tasks)
    handler: src/redis_version/preprocessor-redis.readAndFilterFile
    events:
      - http: 
          path: preprocess-with-redis
          method: post
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      SUBRESULT_FOLDER: ${self:custom.S3_SUBRESULT_FOLDER}
      REGION: ${self:custom.REGION}
      QUEUE_NAME: ${self:custom.SQS_NAME_REDIS}
      REDIS_CONNECTION: ${self:custom.REDIS_CONNECTION}
  calculate-with-redis:
    description: >
      Single lambda - calulator lambda after receiving sqs message,
      fetch cleaned up test data from S3 and calculates GSD index
    handler: src/redis_version/calculator-redis.handler
    events:
      - sqs:
          arn:
            Fn::GetAtt:
              - PreprocessedDataQueueRedis
              - Arn
    environment:
      BUCKET: ${self:custom.S3_BUCKET}
      SUBRESULT_FOLDER: ${self:custom.S3_SUBRESULT_FOLDER}
      PIPELINE_RESULT_FOLDER: ${self:custom.S3_PIPELINE_RESULT_FOLDER}
      REDIS_CONNECTION: ${self:custom.REDIS_CONNECTION}

resources:
  Resources:
    PreprocessedDataQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: ${self:custom.SQS_NAME}
    PreprocessedDataQueueFiFo:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: ${self:custom.SQS_NAME_FIFO}.fifo # must end with fifo suffix
        FifoQueue: true
    PreprocessedDataQueueXRay:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: ${self:custom.SQS_NAME_XRAY}
    PreprocessedDataQueueRedis:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: ${self:custom.SQS_NAME_REDIS}
